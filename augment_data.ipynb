{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"Anger\": 0,\n",
    "    \"Fear\": 1,\n",
    "    \"Happy\": 2,\n",
    "    \"Love\": 3,\n",
    "    \"Sadness\": 4\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    1770\n",
       "4    1202\n",
       "1     920\n",
       "3     809\n",
       "0     699\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"PRDECT-ID Dataset.csv\")  # Adjust the filename\n",
    "data = data[['Customer Review', 'Emotion']]\n",
    "data.rename(columns={'Customer Review': 'text', 'Emotion': 'label'}, inplace=True)\n",
    "data['label'] = data['label'].map(label2id)\n",
    "data['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target sample size\n",
    "target_size = 300\n",
    "\n",
    "# Separate classes\n",
    "data_1 = data[data['label'] == 1]\n",
    "data_2 = data[data['label'] == 2]\n",
    "data_3 = data[data['label'] == 3]\n",
    "data_0 = data[data['label'] == 0]\n",
    "data_4 = data[data['label'] == 4]\n",
    "\n",
    "\n",
    "# Randomly sample from the smaller classes to match the target size\n",
    "data_1_resampled = data_1.sample(n=target_size, replace=True, random_state=42)\n",
    "data_2_resampled = data_2.sample(n=target_size, replace=True, random_state=42)\n",
    "data_3_resampled = data_3.sample(n=target_size, replace=True, random_state=42)\n",
    "data_4_resampled = data_4.sample(n=target_size, replace=True, random_state=42)\n",
    "data_0_resampled = data_0.sample(n=target_size, replace=True, random_state=42)\n",
    "\n",
    "# Combine the resampled data\n",
    "balanced_data = pd.concat([data_1_resampled,\n",
    "                           data_2_resampled,\n",
    "                           data_3_resampled,\n",
    "                           data_4_resampled,\n",
    "                           data_0_resampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(balanced_data['text'], balanced_data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Save train test split\n",
    "train = pd.DataFrame({'text': x_train, 'label': y_train})\n",
    "test = pd.DataFrame({'text': x_val, 'label': y_val})\n",
    "train.to_csv('dataset/train.csv', index=False)\n",
    "test.to_csv('dataset/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Saya sangat senang hari ini\n",
      "Augmented Text:\n",
      "['Saya sangat senang peringati mimapatelarthron']\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "\n",
    "aug = naw.WordEmbsAug(model_type='fasttext',\n",
    "                      model_path=\"wiki.id.vec\",top_k=5)\n",
    "\n",
    "text= \"Saya sangat senang hari ini\"\n",
    "augmented_text = aug.augment(text)\n",
    "\n",
    "print(\"Original:\")\n",
    "print(text) \n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment train\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "train_augmented = train.copy()\n",
    "train_augmented['text'] = train_augmented['text'].apply(lambda x: aug.augment(x))\n",
    "train_augmented.to_csv('dataset/train_augmented.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ulfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
